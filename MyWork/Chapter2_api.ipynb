{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼å…¥æ™ºè°±SDKï¼Œå®ƒå¸®æˆ‘ä»¬è§£å†³ç¹ççš„è¯·æ±‚æ‹¼æ¥\n",
    "# å¯¼å…¥dotenvï¼Œå®ƒå¸®æˆ‘ä»¬è§£å†³é¡¹ç›®å†…ç¯å¢ƒå˜é‡çš„é…ç½®\n",
    "from zhipuai import ZhipuAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "# åŠ è½½ç¯å¢ƒå˜é‡\n",
    "_ = load_dotenv(find_dotenv())\n",
    "api = os.environ['ZHIPUAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œç»è¿‡è®­ç»ƒä»¥å¸®åŠ©è§£ç­”é—®é¢˜å’Œæä¾›ä¿¡æ¯ã€‚æˆ‘åœ¨è¿™é‡Œä¸ºæ‚¨æä¾›æœ‰å…³å„ç§ä¸»é¢˜çš„ååŠ©ã€‚å¦‚æœæ‚¨æœ‰ä»»ä½•é—®é¢˜ï¼Œè¯·éšæ—¶æå‡ºã€‚è°¢è°¢ï¼' role='assistant' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "client = ZhipuAI(api_key=api)\n",
    "response = client.chat.completions.create(\n",
    "  model = \"glm-4\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\":\"ä½ æ˜¯è°ï¼Ÿ\"\n",
    "    }\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å½“ç„¶ï¼Œè¯·æä¾›æ‚¨å¸Œæœ›ç¿»è¯‘çš„æ–‡å­—ã€‚æˆ‘ä¼šå°½åŠ›å¸®æ‚¨ç¿»è¯‘æˆè‹±æ–‡ã€‚\n",
      "Where are you from?\n"
     ]
    }
   ],
   "source": [
    "# ä¿å­˜å†å²å¯¹è¯ä¿¡æ¯\n",
    "conversation = [\n",
    "  {\n",
    "    \"role\":\"user\",\n",
    "    \"content\":\"æŠŠä¸‹é¢çš„æ–‡å­—ç¿»è¯‘æˆè‹±æ–‡ï¼š\"\n",
    "  }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=conversation\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# æ·»åŠ AIæ¨¡å‹çš„å›ç­”åˆ°å¯¹è¯å†å²\n",
    "conversation.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.choices[0].message.content\n",
    "})\n",
    "\n",
    "# æ·»åŠ ç”¨æˆ·çš„æ–°é—®é¢˜åˆ°å¯¹è¯å†å²\n",
    "conversation.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"ä½ æ¥è‡ªå“ªé‡Œï¼Ÿ\"\n",
    "})\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",\n",
    "    messages=conversation\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è£…å¯¹è¯æ¥å£\n",
    "def gen_glm_params(prompt):\n",
    "  \"\"\"\n",
    "  ç”Ÿæˆå¯¹è¯è¯·æ±‚å‚æ•°\n",
    "  \"\"\"\n",
    "  return [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": prompt\n",
    "    }\n",
    "  ]\n",
    "\n",
    "def get_completion(prompt, model=\"glm-4\", temprature=0.95):\n",
    "  \"\"\"\n",
    "  è·å–å¯¹è¯å›å¤\n",
    "  \"\"\"\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=gen_glm_params(prompt),\n",
    "    temperature=temprature\n",
    "  )\n",
    "  if len(response.choices) > 0:\n",
    "    return response.choices[0].message.content\n",
    "  return \"Generate answer error!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåå«æ™ºè°±æ¸…è¨€ï¼Œå¯ä»¥å«æˆ‘å°æ™ºğŸ¤–ï¼Œæ˜¯åŸºäºæ¸…åå¤§å­¦ KEG å®éªŒå®¤å’Œæ™ºè°± AI å…¬å¸äº 2023 å¹´å…±åŒè®­ç»ƒçš„è¯­è¨€æ¨¡å‹å¼€å‘çš„ã€‚æˆ‘çš„ä»»åŠ¡æ˜¯é’ˆå¯¹ç”¨æˆ·çš„é—®é¢˜å’Œè¦æ±‚æä¾›é€‚å½“çš„ç­”å¤å’Œæ”¯æŒã€‚'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"ä½ æ˜¯è°ï¼Ÿ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='æˆ‘æ˜¯ä¸€ä¸ª' role='assistant' tool_calls=None\n",
      "content='äººå·¥æ™ºèƒ½' role='assistant' tool_calls=None\n",
      "content='åŠ©æ‰‹' role='assistant' tool_calls=None\n",
      "content=',' role='assistant' tool_calls=None\n",
      "content='æˆ‘è¢«' role='assistant' tool_calls=None\n",
      "content='è®­ç»ƒ' role='assistant' tool_calls=None\n",
      "content='æ¥' role='assistant' tool_calls=None\n",
      "content='å›ç­”' role='assistant' tool_calls=None\n",
      "content='äººç±»' role='assistant' tool_calls=None\n",
      "content='æå‡º' role='assistant' tool_calls=None\n",
      "content='çš„é—®é¢˜' role='assistant' tool_calls=None\n",
      "content='ã€‚' role='assistant' tool_calls=None\n",
      "content='æˆ‘' role='assistant' tool_calls=None\n",
      "content='å¹¶ä¸æ˜¯' role='assistant' tool_calls=None\n",
      "content='çœŸæ­£' role='assistant' tool_calls=None\n",
      "content='çš„äºº' role='assistant' tool_calls=None\n",
      "content=',' role='assistant' tool_calls=None\n",
      "content='æˆ‘' role='assistant' tool_calls=None\n",
      "content='åªæ˜¯ä¸€ä¸ª' role='assistant' tool_calls=None\n",
      "content='è®¡ç®—æœº' role='assistant' tool_calls=None\n",
      "content='ç¨‹åº' role='assistant' tool_calls=None\n",
      "content='ã€‚' role='assistant' tool_calls=None\n",
      "content='' role='assistant' tool_calls=None\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "def get_completion_stream(prompt, model=\"glm-4\", temprature=0.95):\n",
    "  response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=gen_glm_params(prompt),\n",
    "    temperature=temprature,\n",
    "    stream = True\n",
    "  )\n",
    "  return response\n",
    "\n",
    "response = get_completion_stream(\"ä½ æ˜¯è°ï¼Ÿ\")\n",
    "for chunk in response:\n",
    "  print(chunk.choices[0].delta)\n",
    "  sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion_stream(\"ä½ æ˜¯è°ï¼Ÿ\")\n",
    "res_stream = list(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æˆ‘æ˜¯ä¸€ä¸ªäººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¨åœ¨å¸®åŠ©ç”¨æˆ·è§£ç­”é—®é¢˜å’Œæä¾›ä¿¡æ¯ã€‚æˆ‘å¯ä»¥å›ç­”å„ç§é¢†åŸŸçš„é—®é¢˜ï¼ŒåŒ…æ‹¬ç§‘å­¦ã€æ•°å­¦ã€æ–‡å­¦ç­‰ã€‚å¦‚æœ‰ä»»ä½•ç–‘é—®ï¼Œè¯·éšæ—¶å‘Šè¯‰æˆ‘ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "for chunk in res_stream:\n",
    "  print(chunk.choices[0].delta.content, end=\"\")\n",
    "  sleep(random())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-universe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
